# ğŸŒ™ Luna â€“ Inclusive & AI-first Voice Assistant

**Luna** is a voice assistant designed for people with visual impairments (and later adaptable to other disabilities).  
Its mission is to bring **autonomy, safety, and simplicity** to daily life through a **hybrid open-source architecture** and **AI-first governance**.

---

## âœ¨ Key Features

- ğŸ¤ **Natural conversation** with GPT-4o Realtime (audio â†” audio)  
- ğŸ“ **Hands-free emergency calls** (112 / trusted contacts)  
- â° **Reminders & timers** via voice  
- ğŸ“§ **Gmail integration** (read, summarize, draft replies)  
- ğŸ¶ **Spotify** (playlists, playback control)  
- ğŸ“„ **Document scanning & classification** (OCR â†’ save + smart search)  
- ğŸ“… **Agenda management** (local or Google Calendar sync)  
- ğŸŒ **Web search** adapted for audio-first answers  

---

## ğŸ›¡ï¸ Security & Privacy First

- ğŸ”‘ No API keys stored on device (ephemeral credentials only)  
- ğŸ”’ All local data encrypted (SQLite + system keychain/keystore)  
- ğŸš« Zero voice recordings by default (opt-in storage only)  
- âœ… Critical actions require vocal confirmation (e.g., â€œYes, call emergencyâ€)  

---

## âš¡ Quick Demo

1. Clone & enter the workspace:
   ```bash
   git clone https://github.com/luna-assistant-ai/luna-assistant-ai-core.git
   cd luna-assistant-ai-core
   ```
2. Test and build the core crate:
   ```bash
   cd core
   cargo test
   cargo build
   ```
3. No code yet? Watch [@luna-assistant-ai](https://github.com/luna-assistant-ai) to get notified when the first modules land.

---

## ğŸ› ï¸ Getting Started

1. Clone the repository and enter the workspace:
   ```bash
   git clone https://github.com/luna-assistant-ai/luna-assistant-ai-core.git
   cd luna-assistant-ai-core
   ```
2. Install prerequisites:
   - Rust â‰¥ 1.81 (required)  
   - Xcode for iOS development (required if you target iOS)  
   - Node.js (optional â€” only for local OAuth backend testing)  
   If you do not have Rust yet, run `rustup default stable` after installing rustup.
   > Note: The `core/` crate and client apps will appear as code lands in this repository. If you cloned a skeleton repo without those folders yet, keep an eye on the roadmap section or GitHub issues for the drop.
3. Bootstrap the core crate to verify your toolchain:
   ```bash
   cd core
   cargo test
   cargo build
   ```
4. (Optional) Open the iOS client to run Luna on a simulator:
   ```bash
   open ../clients/ios/App.xcodeproj
   ```
   Then press `Cmd + R` inside Xcode. Refer to the sections below for more platform-specific instructions, environment variables, and integration secrets.
   Additional deep-dive guides will live in the [`luna-assistant-ai-docs`](https://github.com/luna-assistant-ai/luna-assistant-ai-docs) repository as they are published.

**Not ready to code yet?**  
- â­ Star this repo to get notified of first commits  
- ğŸ’¬ Join [Discussions](https://github.com/luna-assistant-ai/luna-assistant-ai-core/discussions) to shape the roadmap  
- ğŸ“– Browse accessibility research and guidelines in [`luna-assistant-ai-docs`](https://github.com/luna-assistant-ai/luna-assistant-ai-docs) (coming soon)  

---

## ğŸ“ Repository Structure

- **[luna-assistant-ai-core](./)** â† you are here (Rust engine, FFI bindings, basic clients)  
- **[luna-assistant-ai-docs](https://github.com/luna-assistant-ai/luna-assistant-ai-docs)** (guides, accessibility standards, prompt library)  
- **[luna-assistant-ai-pro](https://github.com/luna-assistant-ai/luna-assistant-ai-pro)** â€” private (AI copilots, premium integrations, caregiver dashboard)  
- **[@luna-assistant-ai](https://github.com/luna-assistant-ai)** (organization-level roadmap & discussions)  

---

## ğŸ—ï¸ Technical Architecture

### Core (Rust)
- Event bus + intent detection  
- Modular plugins (skills): SOS, reminders, OCR, Gmail, Spotify, search  
- Exposed via FFI (iOS/Android) or service (Raspberry Pi)  

### Clients
- ğŸ“± **iOS (SwiftUI)** â€“ first target  
- ğŸ¤– Android (Jetpack Compose) â€“ planned  
- ğŸ’» Raspberry Pi Hub (mic + speaker + SOS button) â€“ planned  

### Cloud
- **OpenAI GPT-4o Realtime** (audio â†” audio)  
- Lightweight backend (ephemeral keys, OAuth callbacks)  

### Architecture Diagram

```mermaid
flowchart TD
    User["ğŸ‘¤ User (Voice)"]

    subgraph Device["ğŸ“± Device"]
        App["iOS/Android App"]
        Core["ğŸ¦€ Rust Core"]
        Skills["ğŸ”§ Skills\nSOS â€¢ OCR â€¢ Reminders"]
    end

    subgraph Services["â˜ï¸ External Services"]
        GPT["ğŸ¤ GPT-4o Realtime"]
        Gmail["ğŸ“§ Gmail API"]
        Emergency["ğŸš¨ Emergency Services"]
    end

    User <--> App
    App <--> Core
    Core <--> Skills
    Skills --> GPT
    Skills --> Gmail
    Skills --> Emergency

    style User fill:#e1f5fe
    style Skills fill:#f3e5f5
    style Emergency fill:#ffebee
```

*Luna's governance and development process (including AI-assisted analysis) is detailed in the [AI-Assisted Governance](#-ai-assisted-governance-human-in-the-loop) section below.*

---

## ğŸ—ï¸ Architecture Decisions

- **Why Rust core?** Deterministic performance and memory safety for accessibility-critical flows.  
- **Why AI copilots?** Scale expert analysis (compliance, forecasting, benchmarking) in a niche domain with limited human bandwidth.  
- **Why hybrid licensing?** Keep the foundation open-source while funding long-term accessibility R&D through premium integrations.

---

## ğŸ¤– AI-Assisted Governance (Human-in-the-Loop)

Luna leverages AI copilots to scale strategic analysis in accessibilityâ€”a domain where specialized expertise is scarceâ€”while maintaining strict human oversight on all decisions.

### Decision Flow

```
AI Strategic Analysis â†’ Human Review â†’ Community Input â†’ Final Decision
```

**What AI handles:**
- Market intelligence & competitive positioning analysis  
- Product strategy recommendations & feature prioritization  
- Operational modeling & unit economics forecasting  
- Risk assessment & regulatory compliance mapping  

**What humans control:**
- All strategic decisions affecting users and roadmap  
- Safety-critical features (emergency calls, data privacy)  
- Community governance and contributor policies  
- Final product releases and partnership agreements  

### AI Copilot Teams

| Copilot | Strategic Focus | Business-Critical Questions |
|---------|-----------------|-----------------------------|
| **Market Intelligence** | Competitive positioning & user acquisition | "Which accessibility gaps can we own vs. competitors?" "What drives switching from existing solutions?" |
| **Product Strategy** | Feature prioritization & roadmap | "Which skills unlock highest user retention?" "When do we expand beyond visual impairments?" |
| **Operations** | Scalability & unit economics | "At what user volume do we need dedicated infrastructure?" "Which premium features justify subscription pricing?" |
| **Risk Management** | Regulatory compliance & reputation | "What liability exposure from emergency call failures?" "How do privacy regulations limit personalization?" |

### Strategic Output Examples

**Market Intelligence:**  
> "Voiceitt and Be My Eyes dominate communication, not autonomy. Emergency calling gap = 47% of target users underserved. Entry strategy: own the safety vertical first."

**Product Strategy:**  
> "User retention analysis: Emergency features drive 89% retention vs 34% for entertainment features. Prioritize SOS + medication reminders over Spotify integration."

**Operations:**  
> "GPT-4o cost trajectory: $0.08/user at 1K users â†’ $0.03/user at 10K users. Break-even with freemium: 2,500 active users minimum or $15/month premium tier."

**Risk Management:**  
> "Emergency call liability: NZ accident compensation covers technology failures. Key risk = false positives. Mitigation: 3-second confirmation + voice pattern recognition."

### Safeguards in Place

- ğŸ”’ **No autonomous decisions**: All AI outputs require maintainer approval  
- ğŸ‘¥ **Community visibility**: Major decisions discussed in public GitHub discussions  
- ğŸ›¡ï¸ **Safety override**: Human veto on any AI recommendation, no questions asked  
- ğŸ“Š **Transparency**: AI-generated reports tagged and publicly available  

*Think of it as having a strategic consulting team that never sleeps, but your human judgment always has the final say.*

---

## ğŸ”‘ Open Source Strategy

### Hybrid Model
- **Public (open source)**  
  - Rust Core (event bus, intents API)  
  - Minimal iOS/Android clients  
  - Essential skills: SOS, reminders, basic OCR  

- **Private (proprietary)**  
  - AI Copilots (Marketing, Finance, Legal, Tech)  
  - Premium integrations (advanced Gmail, Spotify, LLM-powered search)  
  - Caregiver dashboard (activity insights, opt-in sharing)  

### License Choice
- **Core & clients**: MIT â†’ maximize adoption and contributions  
- **Premium modules**: proprietary license (e.g., SSPL or BUSL to restrict commercial reuse)  

### Community Governance
- **Maintainers**: core team (Rust/iOS/Android devs)  
- **External contributors**: PR â†’ human review required â†’ CI/CD green â†’ auto-merge via Mergify  
- **CONTRIBUTING.md**: clear rules (tests, accessibility, Conventional Commits)  
- **CODE_OF_CONDUCT.md**: inclusivity & respect  
- **GitHub Projects**: public roadmap & prioritization  
- **GitHub Discussions**: open channel for visually impaired users & caregivers  

---

## ğŸ›  Development Workflow

**Project Management:**
- **GitHub Projects** â†’ Public roadmap & feature tracking  
- **GitHub Issues** â†’ Bug reports, accessibility requirements  
- **GitHub Discussions** â†’ Community feedback & early testing signup  

**CI/CD Pipeline:**
- **Drone CI** â†’ Multi-platform builds (iOS, Android, Raspberry Pi Docker images)  
- **Mergify** â†’ Auto-merge when Drone CI passes + review approved  
- **AI Copilots** â†’ Generate PRDs/ADRs/issues directly to GitHub Projects  

---

## ğŸ›£ï¸ Roadmap & Status *(last updated: December 2024)*

- **Weeks 1â€“4**: Core engine scaffold + iOS skeleton â†’ first public commit  
- **Months 1â€“2**: SOS + Reminders skills â†’ closed alpha with testers in NZ  
- **Months 3â€“4**: OCR pipeline + end-to-end voice loop â†’ MVP release and feedback cycle  
- **Post-MVP**: Gmail/Spotify integrations, Raspberry Pi hub, AI governance pilots shared via Docs repo  

Progress snapshots and issues live in GitHub Projects boards and GitHub Discussions.

---

## ğŸ‘©â€ğŸ’» Contributing

Luna thrives on community inputâ€”especially from people with lived accessibility experience. Whether you are new to open source or a seasoned maintainer, we can pair you with a starter issue or mentoring buddy.

- Fork then branch (`feat/...`, `fix/...`)  
- Run tests (`cargo test`, `xcodebuild test`) before opening a PR  
- Fill the PR template (objective, tests, accessibility, rollback)  
- Human review required â†’ CI green â†’ Mergify auto-merge  
- Follow Conventional Commits and keep PRs small, tested, documented  

---

## ğŸ¤ Get Involved

- **Users with visual impairments**: Join early testing via [GitHub Discussions](https://github.com/luna-assistant-ai/luna-assistant-ai-core/discussions)  
- **Developers**: Pick up tasks in [Issues](https://github.com/luna-assistant-ai/luna-assistant-ai-core/issues) and review [CONTRIBUTING.md](./CONTRIBUTING.md)  
- **Organizations & partners**: Start a thread in [GitHub Discussions](https://github.com/luna-assistant-ai/luna-assistant-ai-core/discussions)  
- **Community**: Watch [@luna-assistant-ai](https://github.com/luna-assistant-ai) for release updates  

---

## ğŸ“œ License

- Core: MIT (open source)  
- Basic clients: MIT  
- Premium modules (AI copilots, advanced integrations): proprietary license  
